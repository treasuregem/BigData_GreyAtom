{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "**1. AWS EMR: Introduction**\n",
    "\n",
    "**2. Apache Spark on EMR**\n",
    "\n",
    "**3. AWS S3: Data Storage**\n",
    "\n",
    "**4. Tutorial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS EMR: Introduction\n",
    "\n",
    "## What is AWS EMR?\n",
    "\n",
    "- Amazon Elastic Map Reduce.\n",
    "\n",
    "- Amazon EMR provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data across dynamically scalable Amazon EC2 instances. You can also run other popular distributed frameworks such as Apache Spark, HBase, Presto, and Flink in Amazon EMR, and interact with data in other AWS data stores such as Amazon S3 and Amazon DynamoDB.\n",
    "\n",
    "- Amazon EMR securely and reliably handles a broad set of big data use cases, including log analysis, web indexing, data transformations (ETL), machine learning, financial analysis, scientific simulation, and bioinformatics.\n",
    "\n",
    "- Run a Spark Job with AWS EMR\n",
    "\n",
    "- AWS EMR features: Auto Scaling, Auto termination, Logs to s3, EMR events, Handles hardware failure\n",
    "\n",
    "- Boto3: Run Job Flow call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark on AWS EMR\n",
    "\n",
    "<img src = \"images/Spark-Diagram.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage Layer: Amazon s3\n",
    "\n",
    "### AWS Simple Storage Service\n",
    "\n",
    "Where is data stored?\n",
    "\n",
    "● Scales - just keep putting files, and it will never fill up.\n",
    "\n",
    "● Upload and download your data with SSL encrypted end points\n",
    "\n",
    "● Provides multiple options for encrypting data at rest.\n",
    "\n",
    "● Low Cost - $0.023 per GB\n",
    "\n",
    "● Reading data in a Spark application is as simple as calling -\n",
    "\n",
    "`sc.textFile(“s3n://<bucketname>” )`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How AWS EMR access data from the S3 Bucket?\n",
    "\n",
    "<img src = \"images/spark2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough: Running a spark Job on EMR\n",
    "\n",
    "We'll solve the PageRank problem referring to the spark script prepared in the **Data Proccessing Layer** concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data storage\n",
    "\n",
    "\n",
    "**Spark Codebase:** \n",
    "\n",
    "AWS EMR gives you an option to submit your spark applications through spark scripts stored on AWS S3. ONe can store the whole codebase on S3 and give the cluster access to it.\n",
    "\n",
    "Command to launch a Spark application:\n",
    "\n",
    "`spark-submit <s3_path>`\n",
    "\n",
    "I have created a s3 bucket with name **grey-atom** and the spark scripts are stored in this bucket. Here the code resides in the script `job.py`.\n",
    "\n",
    "**job.py**\n",
    "\n",
    "```python\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from operator import add\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    conf = SparkConf().setAppName(\"Pagerank\")\n",
    "    conf.set('spark.executor.instances', 2)\n",
    "    conf.set('spark.executor.cores', '1')\n",
    "\n",
    "    conf.set('spark.dynamicAllocation.enabled', 'true')\n",
    "    # conf.set('spark.executor.memory', '6')\n",
    "\n",
    "    # conf.set('spark.yarn.executor.memoryOverhead', '4096')\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "    sc.addPyFile('s3://grey-atom/support.py')\n",
    "    from support import parseNeighbors, computeContribs\n",
    "\n",
    "    rdd = sc.textFile(\"s3n://grey-atom/Input/\")\n",
    "    rdd = rdd.map(lambda line: line.split('\\t'))\n",
    "    url_links_rdd = rdd.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "    ranks = url_links_rdd.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "    no_of_iteration = 2\n",
    "    for iteration in range(no_of_iteration):\n",
    "        # Calculates URL contributions to the rank of other URLs.\n",
    "        contribs = url_links_rdd.join(ranks).flatMap(lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "    ranks = ranks.coalesce(99)\n",
    "    ranks.saveAsTextFile('s3n://grey-atom/output')\n",
    "\n",
    "```\n",
    "\n",
    "**Input**\n",
    "\n",
    "The data to process again stored in `Input` in the same s3 bucket.\n",
    "\n",
    "Same is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/s3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS EMR\n",
    "\n",
    "**1. Create a cluster**\n",
    "\n",
    "Go to AWS EMR Colsole and click on `Create Cluster`. Next follow the steps are shown below to launch your cluster:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:**\n",
    "\n",
    "<img src = \"images/one.png\">\n",
    "<img src = \"images/two.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Submit a step\n",
    "\n",
    "Once the Cluster is in ready state, we can go futher to launch a spark application. Next, click on Add step where we just have to give `spark-submit` command. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/step-submit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the Spark Application is running, one can monitor the number of Stages, Jobs, executors that are launched by cluster to execute the Spark aplication. \n",
    "\n",
    "**Click on Application history:**\n",
    "\n",
    "You'll see Jobs, number of Stages, Tasks and various other metrics assciated with the Spark application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/app1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next click on this Job to get more details about Stages**\n",
    "\n",
    "<img src = \"images/app2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output**\n",
    "\n",
    "In the spark-script, I saved my output using the `sc.saveAsTextFile(s3_path)` to the s3 bucket `grey-atom`. Once the step is completed, the output is pushed into the s3 bucket with `output` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/out.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output pushed from Spark Application**\n",
    "\n",
    "<img src = \"images/output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the filenames are like `part-00000`, `part-000001` ... `part-00000n`, these are named by spark only. Essentially we pushed the output from each partition to s3 so the naming follows from that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So this was all about launching EMR clusters to run Spark Applications using AWS S3 as a data storage option.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
