{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "\n",
    "**1. Apache Hive: Introduction**\n",
    "\n",
    "**2. Brief History**\n",
    "\n",
    "**3. Apache Hive use cases**\n",
    "\n",
    "**4. Hands-on**\n",
    "\n",
    "**5 Hive Data Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick up the threads\n",
    "\n",
    "We have covered Data Ingestion and Collector, Data Processing layer till now. Now our data is processed, but question is **How end users will utilize?**. It's very important that One can easily consume the processed data and query it fast. We'll now cover Data Query Layer and Apache Hive is the tool which will help us do wonders with our structured data. Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Hive: Introduction\n",
    "\n",
    "- Apache Hive is a data warehousing tool in the Hadoop Ecosystem, which provides SQL like language for querying and analyzing Big Data. \n",
    "\n",
    "- Hive provides a database query interface to Apache Hadoop.\n",
    "\n",
    "- Hive is used for processing structured data in Hadoop\n",
    "\n",
    "- Hive opens the big data Hadoop ecosystem to nonprogrammers because of its SQL-like capabilities and database-like functionality\n",
    "\n",
    "- Using Hive we don’t need to write complex Map-Reduce jobs, we just need to submit SQL queries. Hive converts these SQL queries into MapReduce jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Hive Tutorial: What is Hive?\n",
    "\n",
    "- Apache Hive is a data warehouse system built on top of Hadoop and is used for analyzing structured and semi-structured data. Hive abstracts the complexity of Hadoop MapReduce.\n",
    "- Basically, it provides a mechanism to project structure onto the data and perform queries written in HQL (Hive Query Language) that are similar to SQL statements. \n",
    "- Internally, these queries or HQL gets converted to map reduce jobs by the Hive compiler. Therefore, you don’t need to worry about writing complex MapReduce programs to process your data using Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive: Who started this Journey?\n",
    "\n",
    "- Before 2008, all the data processing infrastructure in Facebook was built around a data warehouse based on commercial RDBMS. \n",
    "- But, as the data started growing very fast, it became a huge challenge to manage and process this huge dataset. According to a Facebook article, the data scaled from a 15 TB data set in 2007 to a 2 PB data in 2009. \n",
    "- Also, many Facebook products involve analysis of the data like Audience Insights, Facebook Lexicon, Facebook Ads, etc. So, they needed a scalable and economical solution to cope up with this very problem and, therefore started using the Hadoop framework.\n",
    "\n",
    "<img src = \"images/hive_history.png\">\n",
    "\n",
    "\n",
    "\n",
    "### What was the need to build Hive on Hadoop?\n",
    "\n",
    "- Also, for performing simple analysis one has to write a hundred lines of MapReduce code. Since, SQL was widely used by engineers and analysts, including Facebook, therefore, putting SQL on the top of Hadoop seemed a logical way to make Hadoop accessible to users with SQL background.\n",
    "\n",
    "- Hence, the ability of SQL to suffice for most of the analytic requirements and the scalability of Hadoop gave **birth to Apache Hive** that allows to perform SQL like queries on the data present in HDFS.\n",
    "\n",
    "<img src = \"images/hiveworking.jpg\">\n",
    "\n",
    "\n",
    "## Hive: Architecture\n",
    "\n",
    "<img src = \"images/hive_architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive Installation\n",
    "\n",
    "First, install Hadoop then Hive\n",
    "1. https://www.journaldev.com/20358/install-hadoop-on-ubuntu\n",
    "(Note: If the Hadoop download link is not working then use the command - \n",
    "`wget http://mirrors.fibergrid.in/apache/hadoop/common/hadoop-3.0.3/hadoop-3.0.3.tar.gz)`\n",
    "\n",
    "\n",
    "2. https://www.journaldev.com/20353/install-apache-hive-ubuntu-hql-queries\n",
    "\n",
    "Also, if you want to save yourself from installation overhead then I have already done the hard work for you and have created an AWS AMI(OS Image) for you. Just come with a ready to use AWS account and rest I'll take care.\n",
    "\n",
    "AMI ID- ami-05c85a6a17635d1bb\n",
    "\n",
    "\n",
    "### Set warehouse directory path\n",
    "\n",
    "\n",
    "`set hive.metastore.warehouse.dir=/home/ubuntu/hive/warehouse;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to use Apache Hive?\n",
    "\n",
    "- Apache Hive takes advantage of both the worlds i.e. SQL Database System and Hadoop – MapReduce framework. Therefore, it is used by a vast multitude of companies. It is mostly used for data warehousing where you can perform analytics and data mining that does not require real time processing. \n",
    "\n",
    "<img src= \"images/hive_advantages.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's perform some hands-on queries to interact with Hive**\n",
    "\n",
    "- Launch Hive shell by simply typing `hive` into console\n",
    "Let's perform some simple queries:\n",
    "\n",
    "## Databases in Hive\n",
    "\n",
    "- The Hive concept of a database is essentially just a catalog or namespace of tables.\n",
    "\n",
    "- However, they are very useful for larger clusters with multiple teams and users, as a way of avoiding table name collisions. It’s also common to use databases to organize production tables into logical groups.\n",
    "\n",
    "- If you don’t specify a database, the default database is used.\n",
    "\n",
    "- Hive will create a directory for each database. Tables in that database will be stored in subdirectories of the database directory. The exception is tables in the default database, which doesn’t have its own directory.\n",
    "\n",
    "1. Create a database\n",
    "\n",
    "`create database trade;`\n",
    "\n",
    "2. Show available databases\n",
    "\n",
    "`show databases;`\n",
    "\n",
    "3. The USE command sets a database as the working database, analogous to changing working directories in a filesystem:\n",
    "\n",
    "`use trade;`\n",
    "\n",
    "4. **Create a table: ** \n",
    "\n",
    "- The CREATE TABLE statement follows SQL conventions.\n",
    "\n",
    "`CREATE TABLE IF NOT EXISTS trade.stock(DATES STRING, OPEN FLOAT, HIGH FLOAT, LOW FLOAT, CLOSE FLOAT, VOLUME int, ADJCLOSE FLOAT) row format delimited fields terminated by ','' stored as textfile;`\n",
    "\n",
    "**Note:** You can prefix a database name, mydb in this case, if you’re not currently\n",
    "working in the target database.\n",
    "\n",
    "- You can also copy the schema (but not the data) of an existing table:\n",
    "\n",
    "`CREATE TABLE IF NOT EXISTS trade.stock1 LIKE trade.stock;`\n",
    "\n",
    "5. Load data into table\n",
    "\n",
    "`LOAD DATA LOCAL INPATH '/home/ubuntu/data/appl_stock.csv' INTO TABLE stock;`\n",
    "\n",
    "6. Count number of rows\n",
    "\n",
    "`select count (*) from stock;`\n",
    "\n",
    "7. Let's do a map reduce job\n",
    "\n",
    "`select DATES, sum(OPEN) from stock group by DATES;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Hive: Data Model\n",
    "\n",
    "Data in Hive can be categorized into three types on the granular level:\n",
    "\n",
    "1. Table\n",
    "2. Partition\n",
    "3. Bucket\n",
    "\n",
    "\n",
    "**1. Tables:** \n",
    "\n",
    "- Tables in Hive are the same as the tables present in a Relational Database.\n",
    "- One can perform filter, project, join and union operations on them. There are two types of tables in Hive:\n",
    "\n",
    "**a. Managed Table:**\n",
    "\n",
    "- As the name suggests (managed table), Hive is responsible for managing the data of a managed table. \n",
    "\n",
    "- **“Hive manages the data”**, is that if you load the data from a file present in HDFS into a Hive Managed Table and issue a DROP command on it, the table along with its metadata will be deleted. So, the data belonging to the dropped managed_table no longer exist anywhere in HDFS and you can’t retrieve it by any means. Basically, you are moving the data when you issue the LOAD command from the HDFS file location to the Hive warehouse directory.\n",
    "\n",
    "- However, managed tables are less convenient for sharing with other tools. For example, suppose we have data that is created and used primarily by Pig or other tools, but we want to run some queries against it, but not give Hive ownership of the data. We can define an external table that points to that data, but doesn’t take ownership of it.\n",
    "\n",
    "**Command:**\n",
    "\n",
    "`CREATE TABLE <table_name> (column1 data_type, column2 data_type);`\n",
    "\n",
    "**Example -**\n",
    "\n",
    "`create table stock(DATES FLOAT, OPEN FLOAT, HIGH FLOAT, LOW FLOAT, CLOSE FLOAT, VOLUME int, ADJCLOSE FLOAT) row format delimited fields terminated by ','' stored as textfile;`\n",
    "\n",
    "\n",
    "`LOAD DATA INPATH <HDFS_file_location> INTO table managed_table;`\n",
    "\n",
    "\n",
    "- Let's issue a drop commands to see if data is lost or not:\n",
    "\n",
    "`drop table stock;`\n",
    "\n",
    "Let's check warehouse directory- \n",
    "\n",
    "`hadoop fs -ls /home/ubuntu/hive/warehouse/trade.db/`\n",
    "\n",
    "\n",
    "**b. External Table:\n",
    "\n",
    "**Command:**\n",
    "\n",
    "`CREATE EXTERNAL TABLE <table_name> (column1 data_type, column2 data_type) LOCATION ‘<table_hive_location>’;`\n",
    "\n",
    "`LOAD DATA INPATH ‘<HDFS_file_location>’ INTO TABLE <table_name>;`\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's first place the data at correct location - \n",
    "\n",
    "`hadoop fs -mkdir /home/ubuntu/ext_data/`\n",
    "\n",
    "`hadoop fs -put appl_stock.csv /home/ubuntu/ext_data/`\n",
    "\n",
    "Now run Hive query to create external table:\n",
    "\n",
    "`create external table stock_ext(DATES STRING, OPEN FLOAT, HIGH FLOAT, LOW FLOAT, CLOSE FLOAT, VOLUME int, ADJCLOSE FLOAT) row format delimited fields terminated by ',' LOCATION '/home/ubuntu/ext_data/';`\n",
    "\n",
    "For external table, Hive is not responsible for managing the data. In this case, when you issue the LOAD command, Hive moves the data into its warehouse directory. Then, Hive creates the metadata information for the external table. Now, if you issue a DROP command on the external table, only metadata information regarding the external table will be deleted. Therefore, you can still retrive the data of that very external table from the warehouse directory using HDFS commands.\n",
    "\n",
    "\n",
    "Let's check if the table is listed in warehouse dir\n",
    "\n",
    "`hadoop fs -ls /home/ubuntu/hive/warehouse/trade.db`\n",
    "\n",
    "So no entry for this table is created in warehouse dir. Let's drop this table now to see if data is lost from hadoop directory too\n",
    "\n",
    "`drop table stock_ext;`\n",
    "\n",
    "Now let's have look to the data directory:\n",
    "\n",
    "`hadoop fs -ls /home/ubuntu/ext_data`\n",
    "\n",
    "\n",
    "**2. Partitions:\n",
    "Command:**\n",
    "\n",
    "`CREATE TABLE table_name (column1 data_type, column2 data_type) PARTITIONED BY (partition1 data_type, partition2 data_type,….);`\n",
    "\n",
    "Hive organizes tables into partitions for grouping similar type of data together based on a column or partition key. Each Table can have one or more partition keys to identify a particular partition. This allows us to have a faster query on slices of the data.\n",
    "\n",
    "**Note**: Remember, the most common mistake made while creating partitions is to specify an existing column name as a partition column. While doing so, you will receive an error – “Error in semantic analysis: Column repeated in partitioning columns”.\n",
    "\n",
    "- Let us understand partition by taking an example where I have a table **student_details** containing the student information of some engineering college like student_id, name, department, year, grades, etc.\n",
    "\n",
    "- Now, if I perform partitioning based on department column, the information of all the students belonging to a particular department will be stored together in that very partition. Physically, a partition is nothing but a sub-directory in the table directory.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Let’s say we have data for three departments in our student_details table – CSE, ECE and Civil.\n",
    "\n",
    "- Therefore, we will have **three partitions** in total for each of the departments as shown in the image below. And, for each department we will have all the data regarding that very department residing in a separate sub – directory under the Hive table directory. \n",
    "\n",
    "- For example, all the student data regarding CSE departments will be stored in **user/hive/warehouse/student_details/dept.=CSE**.\n",
    "\n",
    "- So, the queries regarding CSE students would only have to look through the data present in the CSE partition. This makes partitioning very useful as it reduces the query latency by scanning only relevant partitioned data instead of the whole data set. In fact, in real world implementations, you will be dealing with hundreds of TBs of data. \n",
    "\n",
    "- So, imagine scanning this huge amount of data for some query where 95% data scanned by you was un-relevant to your query.\n",
    "\n",
    "\n",
    "**3. Buckets:\n",
    "Commands:**\n",
    "\n",
    "`CREATE TABLE table_name PARTITIONED BY (partition1 data_type, partition2 data_type,….) CLUSTERED BY (column_name1, column_name2, …) SORTED BY (column_name [ASC|DESC], …)] INTO num_buckets BUCKETS;`\n",
    "\n",
    "- Now, you may **divide each partition or the unpartitioned table into Buckets based on the hash function of a column in the table**. \n",
    "\n",
    "- Actually, each bucket is just a file in the partition directory or the table directory (unpartitioned table). Therefore, if you have chosen to divide the partitions into n buckets, you will have n files in each of your partition directory.\n",
    "\n",
    "- For example, you can see the above image where we have bucketed each partition into 2 buckets. So, each partition, say CSE, will have two files where each of them will be storing the CSE student’s data.\n",
    "\n",
    "**How Hive distributes the rows into buckets?**\n",
    "\n",
    "Well, Hive determines the bucket number for a row by using the formula: **hash_function (bucketing_column) modulo (num_of_buckets)**. Here, hash_function depends on the column data type. \n",
    "\n",
    "**For example**:\n",
    "\n",
    "- if you are bucketing the table on the basis of some column, let’s say user_id, of INT datatype, the hash_function will be – hash_function (user_id)= integer value of user_id. \n",
    "\n",
    "- And, suppose you have created two buckets, then Hive will determine the rows going to bucket 1 in each partition by calculating: (value of user_id) modulo (2). Therefore, in this case, rows having user_id ending with an even integer digit will reside in a same bucket corresponding to each partition.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/buckets.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reality \n",
    "\n",
    "Still, Hive is not really a data warehouse. It's not really even a database. You can build and design a data warehouse with Hive, and you can build and design database tables with Hive, but certain limitations exist that require many workarounds and will pose challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a hadoop cluster, should hive be installed on all nodes?\n",
    "Hive SQL will get converted to MapReduce jobs and we don't have to submit MapReduce job from all node in a Hadoop cluster, in the same way we don't need Hive to be installed in all node of Hadoop cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
