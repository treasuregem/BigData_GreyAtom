{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "**1. What is Big Data?**\n",
    "\n",
    "**2. Big Data: Brief history**\n",
    "\n",
    "**3. Big Data: Real-World challanges**\n",
    "\n",
    "**4. Sources of Big Data**\n",
    "\n",
    "**5. Characterstics of Big Data**\n",
    "\n",
    "**6. Big Data Architectures**\n",
    "\n",
    "**7. Programming Model for Big Data** \n",
    "\n",
    "**8. Quiz**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Big Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Big Data is changing our world***\n",
    "\n",
    "Big Data journey started with the massive discharge in the volume of data generated since the dawn of the digital age. Data itself isn’t a new innovation but performing operations on data can help us do wonders. Earlier we had any data share in paper transaction records but with the arrival of digital age, we can easily store large amount of data in systematic way and is accessible to the world. \n",
    "\n",
    "We produce data whenever we go online, when we carry our GPS-equipped smartphones, when we share our activities on social media, and whenever the internet proves to be a source of entertainment/information for us. .Every digital action is accompanied by data generation. \n",
    "The term **Big Data** refers to the collection of all this data which is growing day by day and our ability to use it to our advantage across a wide range of areas and doing business.\n",
    "\n",
    "<img src=\"images/BigData.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Big Data: Brief history\n",
    "\n",
    "- Let’s go through the history of big data to see how we’ve gotten to where we are! \n",
    "\n",
    "- We haven’t always had the infrastructure needed for handling big data, in terms of network speed and storage capacity. One of the first companies which had both the capacity for big data and the need for it was Google.\n",
    "\n",
    "- Another was Yahoo. One of Yahoo’s search engineers, Doug Cutting, created Lucene in 1999. The project ran well for a while but was running into a few difficulties, and Google happened to release a relevant paper on a distributed filesystems, which was then integrated into Lucene.\n",
    "\n",
    "- Again in 2004, Google released a paper about a framework called MapReduce, and then it was integrated into some of Yahoo’s infrastructure. In 2006, this integration was pulled out into its own project, called **Hadoop**. \n",
    "\n",
    "- The Hadoop ecosystem developed over time and eventually, some very smart folks at Berkeley created **Spark**, which is basically the de facto big data processing framework now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Big Data: Real-World challanges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data: Why and Where?\n",
    "\n",
    "\n",
    "### Why?\n",
    "- Big Data operates on the system that the more you understand about anything or any situation, the more presumably you can gain new insights and offer predictions about what will happen in the future. \n",
    "\n",
    "- By examining more data points, connections begin to emerge that were previously hidden, and these connections facilitate us to learn and make intelligent and quicker decisions.\n",
    "\n",
    "### Where?\n",
    "\n",
    "Before getting into any technical details of Building Big Data Architectures, it is good to learn how Big Data is helping various industries in improving their productivity. Below are some brief case studies which may help us to come up with some innovative ideas during this course:\n",
    "\n",
    "\n",
    "1. **Facebook**: Have you ever seen one of the videos on Facebook that shows a “flashback” of posts, likes, or images—like the ones you might see on your birthday, or on the anniversary of becoming friends with someone? If so, you have seen examples of how Facebook uses Big Data. Let's take example of **The Flashback** - Honoring its 10th anniversary, Facebook offered its users the option of viewing and sharing a video that traces the course of their social network activity from the date of registration till the present. Called the “Flashback,” this video is a collection of photos and posts that received the most comments and likes and set to a nostalgic background music. Imagine so much data for each user will sum up to something we refer **Big Data** when it coms to all users.\n",
    "\n",
    "<img src = \"images/facebook.jpg\">\n",
    "\n",
    "\n",
    "\n",
    "2. **OYO**: Customise bookings, go through user preferences, and offer rooms to match what customers actually like. For example, the company can keep a tab on the preferred breakfast timings for its customers and serve them accordingly on each booking. Similarly, user preferences for room types, such as those with a window or a balcony, are offered during every booking\n",
    "\n",
    "\n",
    "3. **NETFLIX**: Having drawn in millions of users with its high-quality original programming, is now using its trove of data and analytics about international viewing habits to create and buy programming that it knows will be embraced by large, ready-made audiences.\n",
    "\n",
    "\n",
    "4. **UBER**: Uber is cutting the number of cars on the roads of London by a third through UberPool that cater to users who are interested in lowering their carbon footprint and fuel costs. Uber’s business is built on Big Data, with user data on both drivers and passengers fed into algorithms to find suitable and cost-effective matches, and set fare rates. Fares are calculated automatically, using GPS, street data and the company’s own algorithms which make adjustments based on the time that the journey is likely to take.\n",
    "\n",
    "\n",
    "5. **BANK OF AMERICA**: “BankAmeriDeals” provides cash-back offers to credit and debit-card customers based upon analyses of their prior purchases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sources of Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When a company generates data, owns and controls it, this data is **internal**. \n",
    "- **External data** is public data or the data generated outside the company; correspondingly, the company neither owns nor controls it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sources.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Characterstics of Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V's of Big Data\n",
    "\n",
    "- **Volume**\n",
    "- **Variety**\n",
    "- **Veloctiy**\n",
    "- **Veracity**\n",
    "- **Value**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1. Volume**\n",
    "Volume refers to the amount of data (Size of the data).\n",
    "\n",
    "- Let's have a look to the amount of data Twitter produce each second. Intrestingly, there is a website which gives us an estimate count. \n",
    "\n",
    "- If we take all the data generated in the world between the beginning of time and the year 2000, it is the same amount we now generate every minute!\n",
    "\n",
    "- With Big Data technology, we can now think of designing architectures to process such large volume of data.\n",
    "\n",
    "- Go to http://www.internetlivestats.com/one-second/#tweets-band and see how many tweets are prduced in a minute. Every second, on average, around 6,000 tweets are tweeted on Twitter (visualize them here), which corresponds to over 350,000 tweets sent per minute, 500 million tweets per day and around 200 billion tweets per year.\n",
    "\n",
    "- An orginzation can easily use these tweets to a Sentiment analysis on the service/product they are selling and understand their customer needs better. \n",
    "\n",
    "**Google it - Sentiment Analysis on twitter data** \n",
    "\n",
    "*You'll se n number of results for the same! Yes, people around world is already doing it. :) *\n",
    "\n",
    "- With this let's talk about google search, 40,000 search queries are performed per second (on Google alone), which makes it 3.46 million searches per day and 1.2 trillion every year.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2. Variety**\n",
    "\n",
    "Variety refers to the many sources and types of data.\n",
    "\n",
    "-  Data today comes in many different formats: structured data, semi-structured data, unstructured data and even complex structured data.\n",
    "\n",
    "- **Structured Data** : Any data that can be stored, accessed and processed in the form of fixed format is termed as a structured data. Structured data refers to kinds of data with a high level of organization, such as information in a relational database. E.g. web log data, Click-stream data, Sensor data.\n",
    "\n",
    "\n",
    "- **Unstructured Data** : Unstructured data files often include text and multimedia content. Examples include e-mail messages, word processing documents, videos, photos, audio files, presentations, webpages and many other kinds of business documents.\n",
    "\n",
    "\n",
    "- **Semi-structured Data** : The data does not reside in fixed fields or records, but does contain elements that can separate the data into various hiearchies With some process you can store them in relation database. Examples of semi-structured: XML, .CSV and JSON documents.\n",
    "\n",
    "It goes without say that the structured data is easier to upload, extract, load, store, query and analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Velocity\n",
    "\n",
    "Volume and variety are important, but velocity of big data also has a large impact on businesses. Velocity refers to the speed at which new data is generated and the speed at which data moves around. \n",
    "\n",
    "- Analyzing data quickly can alert businesses to stocking issues fast so the problem can be solved before it gets worse. It can also speed up the decision making process to keep up with market changes. \n",
    "\n",
    "-  Big data technology now allows us to analyze the data while it is being generated without ever putting it into databases.\n",
    "\n",
    "- Facebook users upload more than 900 million photos a day. Facebook has to handle a tsunami of photographs every day. It has to ingest it all, process it, file it, and somehow, later, be able to retrieve it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Veracity\n",
    "\n",
    "Veracity refers to the messiness or trustworthiness of the data. We've talked about photographs, sensor data, tweets, encrypted packets, and so on. Each of these are very different from each other.\n",
    "\n",
    "-  This data isn't the old rows and columns and database joins of our forefathers. It's very different from application to application, and much of it is unstructured. \n",
    "\n",
    "- Photos and videos and audio recordings and email messages and documents and books and presentations and tweets and ECG strips are all data, but they're generally unstructured, and incredibly varied.\n",
    "\n",
    "***All that data diversity makes up the variety vector of big data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Value\n",
    "\n",
    "Value refers to our ability turn our data into value. \n",
    "\n",
    "- Before an organization joins the rush to spend yet more on big data, it is salutary to consider what really works and what management capabilities your organization needs to be able to actually extract value from big data.\n",
    "\n",
    "<img src = \"images/value.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Big Data Architectures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example, a company wants to start building up **Big Data architectures**.\n",
    "\n",
    "**What can be a reason behind this shift?**\n",
    "\n",
    "Companies nowadays collect a tremendous amount of data on their customers. From purchase history to social media commentary, customer insights may be collected across multiple touchpoints. In addition, contact center metrics such as average handling time and first contact resolution provide data on how the customer experience is affected by service practices. The task here is to take their Website logs and predict customer behaviour of consuming their services. Say for example, answering question like, how much time on an average a customer spents there, what products are hit most and where do their search ends.\n",
    "\n",
    "**What tasks you think are invovled in building an architecture for handling this Big Data problem?**\n",
    "\n",
    "The best way to propose a solution to a Big Data problem is to divide it into layers of operations. This course will describe on desigining Big Data Pipeline which itself consists of layers as follows:\n",
    "\n",
    "1. Data Ingestion Layer\n",
    "2. Data Collector Layer\n",
    "3. Data Processing Layer\n",
    "4. Data Storage Layer\n",
    "5. Data Query Layer\n",
    "6. Data Visualization Layer\n",
    "\n",
    "Let's have a quick look to all the layers so that we can start up on building one of ours. Detailed explanation for each layer will be provided in their respective section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Big Data layerd architecture](images/layers.png \"Big Data layerd architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data Ingestion Layer**\n",
    "\n",
    "Big Data Ingestion involves connecting to various data sources, extracting the data, and detecting the changed data. It's about moving data - and especially the unstructured data - from where it is originated, into a system where it can be stored and analyzed.\n",
    "\n",
    "**2. Data Collector Layer**\n",
    "\n",
    "Data Collector layer comes into play when there are multiple sources ingesting Data into your pipeline. Before starting any analysis it's necessary to collect all data sources into one and bring to a format which further pipeline can take easily without making the architecture complex. This can only happen when all the data ingestion is finally being collected at one place. Essentialy the focus is on the transportation of data from ingestion layer to rest of data pipeline. \n",
    "\n",
    "**3. Data Processing Layer**\n",
    "\n",
    "This is layer where a Data Scientist can play with data as much desired. Keeping this in mind, the course will take up this layer from basic to advance level. Now, we have all the data in required format and next we need to design parallel processing on the data and  get some output to serve our purpose of the Data Analysis done so far. Workflow is very simple - Input --> Process --> Output.\n",
    "\n",
    "**4. Data Storage Layer**\n",
    "\n",
    "Storage becomes a challenge when the size of the data you are dealing with, becomes large. Several possible solutions can rescue from such problems. Finding a storage solution is very much important when the size of your data becomes large. This layer focuses on **\"where to store such a large data in an efficient, scalable, easily accesible and secured manner\"**.\n",
    "\n",
    "**5. Data Query Layer**\n",
    "\n",
    "This is the layer where strong analytic processing takes place. Data analytics is an\n",
    "essential step which solved the inefficiencies of traditional data platforms to handle\n",
    "large amounts of data related to interactive queries, ETL, storage and processing \n",
    "\n",
    "**6. Data Visualization Layer**\n",
    "\n",
    "This layer focus on Big Data Visualization. We need something that will grab\n",
    "people’s attention, pull them in, make your findings well-understood. This is the\n",
    "where the data value is perceived by the user.\n",
    "\n",
    "• **Dashboards** – Save, share, and communicate insights. It helps users generate\n",
    "questions by revealing the depth, range, and content of their data stores.\n",
    "\n",
    "• **Recommenders** - Recommender systems focus on the task of information\n",
    "filtering, which deals with the delivery of items selected from a large collection\n",
    "that the user is likely to find interesting or useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Programming Models for Big Data\n",
    "\n",
    "## Why do we need a Programming model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the large volume of data, applications that work on big data need to distribute data on a cluster of processors, and processing has to be carried out in parallel for computation to complete in a reasonable amount of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the challenge for developers?\n",
    "\n",
    "\n",
    "Distributed applications require a developer to orchestrate concurrent computation and communication across machines, in a manner that is robust to delays and failures. This adds a overhead of maintaining the infrastructure demands and give considerate time to deploying the application and not just developing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the solution?\n",
    "\n",
    "- A programmer is provided high level primitives (API) to express computation tasks;\n",
    "- The model automatically parallelize the tasks and executes them on a cluster of shared nothing commodity compute nodes.\n",
    "- Focus on the solution of the problem rather than the mundane tasks of parallelization\n",
    "\n",
    "### Define: Programming Model for Big Data\n",
    "\n",
    "- A programming model is an abstraction or existing machinery or infrastructure. It is a set of abstract runtime libraries and programming languages that form a model of computation.\n",
    "\n",
    "- If the enabling infrastructure for big data analysis is distributed file systems, then the programming model for big data should enable the programmability of the operations within distributed file systems.\n",
    "\n",
    "- Enable a developer to write computer programs that work efficiently on top of distributed file systems using big data.\n",
    "\n",
    "-  In big data programming, users focus on writing data-driven parallel programs which can be executed on large scale and distributed environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll see three major programming models for writing big data applications:\n",
    "\n",
    "## 7.1 Distributed File System\n",
    "\n",
    "## 7.2 MapReduce\n",
    "\n",
    "## 7.3 Functional Programming\n",
    "\n",
    "\n",
    "Let's dig into details\n",
    "\n",
    "### 7.1. Distributed File System\n",
    "\n",
    "- In a distributed file system, Data sets, or parts of a data set, can be replicated across the multiple nodes of a cluster.\n",
    "\n",
    "- Distributed file systems replicate the data between the racks, and also computers distributed across geographical regions.\n",
    "\n",
    "- Since data is already on these nodes, then analysis of parts of the data is needed in a data parallel fashion, computation can be moved to these nodes.  \n",
    "- Data replication makes the system more fault tolerant.\n",
    "\n",
    "- That means, if some nodes or a rack goes down, there are other parts of the system, the same data can be found and analyzed.\n",
    "\n",
    "- Data replication also helps with scaling the access to this data by many users.\n",
    "\n",
    "**How the data is distributed and replicated in a cluster?**\n",
    "\n",
    "- Consider a cluster of servers, basically a group of machines which we will refer as a cluster of nodes. In this cluster we are referring one server as a master server. Let's look how this grouping take place in hadoop cluster.\n",
    "\n",
    "- Hadoop DFS has a master/slave architecture. An HDFS cluster consists of a **single NameNode**, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are **a number of DataNodes** , usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.\n",
    "\n",
    "- In Hadoop, **Backup node** keeps an in-memory, up-to-data copy of the file system namespace, which is always synchronized with the active NameNode state.\n",
    "\n",
    "\n",
    "<img src = \"images/namenode.jpg\">\n",
    "\n",
    "**Don't get confused between NameNode and DataNode!**\n",
    "\n",
    "- **NameNode** - NameNode is also known as the Master. NameNode only stores the metadata of HDFS – the directory tree of all files in the file system, and tracks the files across the cluster.\n",
    "- **DataNode** - DataNode is also known as the Slave. DataNode is responsible for storing the actual data in HDFS. When a DataNode is down, it does not affect the availability of data or the cluster. NameNode will arrange for replication for the blocks managed by the DataNode that is not available.\n",
    "\n",
    "\n",
    "#### A simple use case\n",
    "\n",
    "- Suppose you have enormous amount of data generated on a regular basis and so you need to store it somewhere. There are two ways to approach this problem. \n",
    "    \n",
    "    1. First one, a straight approach to store it on a big storage capacity node, also called **vertical scaling**.\n",
    "    2. The second one is to store it on a collection of nodes, also called **horizontal scaling**.\n",
    "    \n",
    "    **What if we want to store more data?** \n",
    "    \n",
    "    \n",
    "        1.  Buy more storage. But remember, nothing is infinite. For instance, Facebook had a storage of 300 petabytes of data in 2014. Can you think of scaling up to this limit with a hard drive?\n",
    "    \n",
    "        2.  Here Horizontal scaling and Distributed file system can together help us. Need to store more data? Let's take a cluster of 1000 nodes but here is a good probability that each node will get out service once in three years on an average.Thus with a cluster of 1,000 nodes you will get one pillar each day, approximately which may lead to data loss. That is where the replication property of distributed file system save us from data loss. Each scaling approach has its own pros and cons. Accessing data you usually get lower latency with vertical scaling. You get higher latency with horizontal scale but you can build a bigger storage solution. \n",
    "        \n",
    "        \n",
    "        \n",
    "<img src=\"images/scaling.png\">\n",
    "\n",
    "***Fig: Vertical VS Horizontal Scaling***\n",
    "\n",
    "\n",
    "\n",
    "***Or we can think of this in a non-technical fashion as shown below:***\n",
    "\n",
    "\n",
    "<img src=\"images/scaling.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 MapReduce\n",
    "- Enables writing data-centric parallel applications.\n",
    "- MapReduce is inspired by the commonly used functions - Map and Reduce in combination with the **divide-and-conquer parallel paradigm**. \n",
    "- For a single MapReduce job, users implement two basic procedure objects **Mapper and Reducer** for different processing stages as shown in Figure below.\n",
    "- Then the MapReduce program is automatically interpreted by the execution engine and executed in parallel in a distributed environments.\n",
    "- **Key-Value based**: In MapReduce, both input and output data are considered as Key-Value pairs with different types.\n",
    "\n",
    "#### A simple use case\n",
    "\n",
    "- Suppose you have a dataset of videos getting upload to Youtube and you aim to find out what are the top 5 categories with maximum number of videos uploaded. How will you proceed?\n",
    " \n",
    " \n",
    "- Now from the mapper, we can get the video category from each video and map it into a key-value pair form with video category as key and related value ‘1’ as values which will be passed to the reducer step next.\n",
    "\n",
    "\n",
    "- Reducer can finally aggregate the data at one place and map the count to each video category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Map Reduce example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/MapReduce.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Functional Programming\n",
    "\n",
    "- In functional programming, programming interfaces are specified as functions that applied on input data sources.\n",
    "\n",
    "- The computation is treated as a calculation of functions.\n",
    "\n",
    "- Functional programming itself is declarative and it avoids mutable states sharing. Compared to Object-oriented Programming it is more compact and intuitive for representing data driven transformations and applications\n",
    "\n",
    "- Wide application will be seen while working with Apache Spark. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/funcProg.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Quiz\n",
    "\n",
    "**Q 1 - The source of HDFS architecture in Hadoop originated as**\n",
    "\n",
    "A - Google distributed filesystem\n",
    "\n",
    "B - Yahoo distributed filesystem\n",
    "\n",
    "C - Facebook distributed filesystem\n",
    "\n",
    "D - Azure distributed filesystem\n",
    "\n",
    "**Answer : A**\n",
    "\n",
    "**Q.2 What fact is more relevant to the horizontal scaling of the filesystems than to the vertical scaling?**\n",
    "\n",
    "A.Usage of commodity hardware \n",
    "\n",
    "B. A simple structure\n",
    "\n",
    "C. It provides a lower latency than the other type of scaling\n",
    "\n",
    "**Answer: A**\n",
    "\n",
    "**Q.3 Which of the following are true about Hadoop?**\n",
    "\n",
    "A. Open Source\n",
    "\n",
    "B. Distributed Processing Framework\n",
    "\n",
    "C. Distributed Storage Framework\n",
    "\n",
    "D. All of these\n",
    "\n",
    "**Answer: D**\n",
    "\n",
    "**Q.4 Which of the following are false about Hadoop?**\n",
    "\n",
    "A. Hadoop works in Master-Slave fashion\n",
    "\n",
    "B. Master & Slave both are worker nodes\n",
    "\n",
    "C. User submit his work on master, which distribute it to slaves\n",
    "\n",
    "D. Slaves are actual worker node\n",
    "\n",
    "**Answer - B**\n",
    "\n",
    "**Q.5 Which of following is the programming model designed for processing large volumes of data in parallel by dividing the work into a set of independent tasks.**\n",
    "\n",
    "A. Hive\n",
    "\n",
    "B. Pig\n",
    "\n",
    "C. MapReduce\n",
    "\n",
    "D. HDFS\n",
    "\n",
    "**Answer - C**\n",
    "\n",
    "\n",
    "**Q.6 Which statement is true about DataNode?**\n",
    "\n",
    "A. It is the actual worker node that stores meta data.\n",
    "\n",
    "B. It is the slave node that stores metadata.\n",
    "\n",
    "C. It is the Master node that stores actual data.\n",
    "\n",
    "D. It is the slave node that stores actual data.\n",
    "\n",
    "**Answer - D**\n",
    "\n",
    "**Q.7 What is a Metadata in Hadoop?**\n",
    "\n",
    "A. Data stored by user\n",
    "\n",
    "B. Information about the data stored in datanodes\n",
    "\n",
    "C. User information\n",
    "\n",
    "D. None of these\n",
    "\n",
    "**Answer - B**\n",
    "\n",
    "**Q.8 Choose the correct statement?**\n",
    "\n",
    "A. Master assigns work to all the slaves\n",
    "\n",
    "B. Client need to interact with master first, as it is the single place where all the meta data is available\n",
    "\n",
    "C. We cannot edit data once written in Hadoop\n",
    "\n",
    "D. All of these\n",
    "\n",
    "**Answer -D**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
